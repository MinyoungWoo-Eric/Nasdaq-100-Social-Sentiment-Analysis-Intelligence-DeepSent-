# report_utils.py
import os
import uuid
import shutil 
import time
from datetime import datetime, date
from typing import List, Dict, Any, Optional
from collections import defaultdict
from functools import wraps
import pandas as pd
import numpy as np

# ============ Fix Azure OpenAI proxy bug ============
import warnings
warnings.filterwarnings("ignore")

import openai
from openai._base_client import SyncHttpxClientWrapper, AsyncHttpxClientWrapper
from openai import InternalServerError, APIError, RateLimitError

class FixedSyncClient(SyncHttpxClientWrapper):
    def __init__(self, *args, **kwargs):
        kwargs.pop("proxies", None)
        super().__init__(*args, **kwargs)

class FixedAsyncClient(AsyncHttpxClientWrapper):
    def __init__(self, *args, **kwargs):
        kwargs.pop("proxies", None)
        super().__init__(*args, **kwargs)

openai._base_client.SyncHttpxClientWrapper  = FixedSyncClient
openai._base_client.AsyncHttpxClientWrapper = FixedAsyncClient

CHAT_DEPLOYMENT = "gpt-4o"
EMBEDDING_DEPLOYMENT = "text-embedding-ada-002"

# ============ æ ¸å¿ƒä¼˜åŒ–ï¼šé‡è¯•+èŠ‚æµè£…é¥°å™¨ ============
def retry_on_azure_error(max_retries: int = 5, delay: float = 3.0, backoff: float = 1.5):
    """
    è£…é¥°å™¨ï¼šAzure OpenAIè°ƒç”¨å¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•
    :param max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    :param delay: åˆå§‹å»¶è¿Ÿï¼ˆç§’ï¼‰
    :param backoff: å»¶è¿Ÿå€æ•°ï¼ˆæ¯æ¬¡é‡è¯•å»¶è¿Ÿ*backoffï¼‰
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            retries = 0
            current_delay = delay
            while retries < max_retries:
                try:
                    return func(*args, **kwargs)
                except (InternalServerError, APIError, RateLimitError) as e:
                    retries += 1
                    if retries >= max_retries:
                        print(f"âŒ Max retries ({max_retries}) reached for {func.__name__}: {e}")
                        raise
                    print(f"âš ï¸ Azure API error (retry {retries}/{max_retries}): {e}. Retrying in {current_delay:.1f}s...")
                    time.sleep(current_delay)
                    current_delay *= backoff
                except Exception as e:
                    print(f"âŒ Non-retryable error in {func.__name__}: {e}")
                    raise
            return None
        return wrapper
    return decorator

def throttle(seconds: float = 1.0):
    """
    è£…é¥°å™¨ï¼šé™åˆ¶å‡½æ•°è°ƒç”¨é¢‘ç‡ï¼ˆæ¯æ¬¡è°ƒç”¨é—´éš”è‡³å°‘secondsç§’ï¼‰
    :param seconds: æœ€å°è°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰
    """
    def decorator(func):
        last_called = 0.0
        @wraps(func)
        def wrapper(*args, **kwargs):
            nonlocal last_called
            elapsed = time.time() - last_called
            if elapsed < seconds:
                sleep_time = seconds - elapsed
                print(f"â³ Throttling {func.__name__}, sleeping {sleep_time:.1f}s...")
                time.sleep(sleep_time)
            result = func(*args, **kwargs)
            last_called = time.time()
            return result
        return wrapper
    return decorator

# ============ æ ¸å¿ƒä¼˜åŒ–ï¼šç»Ÿä¸€Chromaä¸´æ—¶ç›®å½•ç®¡ç† ============
CHROMA_ROOT_DIR = "./chroma_temp_root"
os.makedirs(CHROMA_ROOT_DIR, exist_ok=True)

import tempfile

def get_unique_chroma_dir(prefix: str = "vector_db") -> str:
    """
    åœ¨ç³»ç»Ÿä¸´æ—¶ç›®å½•ä¸­åˆ›å»ºå”¯ä¸€çš„Chromaç›®å½•
    Streamlit Cloud æ–‡ä»¶ç³»ç»Ÿä¸´æ—¶ä¸”åªè¯»ï¼Œé‡å¯åè‡ªåŠ¨æ¸…ç†ï¼Œæ— éœ€æ‰‹åŠ¨ç®¡ç†
    """
    # ä½¿ç”¨ç³»ç»Ÿä¸´æ—¶ç›®å½•ï¼Œæ¯æ¬¡è¿è¡Œç‹¬ç«‹ï¼Œé¿å…å†²çª
    temp_root = tempfile.mkdtemp(prefix="chroma_")
    unique_id = str(uuid.uuid4())[:8]
    unique_dir = os.path.join(temp_root, f"{prefix}_{unique_id}")
    os.makedirs(unique_dir, exist_ok=True)
    return unique_dir

def clean_chroma_temp_dirs(keep_latest: int = 0):
    """
    æ¸…ç†Chromaä¸´æ—¶ç›®å½•ï¼ˆå¯é€‰ä¿ç•™æœ€æ–°çš„Nä¸ªï¼‰
    :param keep_latest: ä¿ç•™æœ€æ–°çš„ç›®å½•æ•°é‡ï¼Œ0è¡¨ç¤ºå…¨éƒ¨æ¸…ç†
    """
    if not os.path.exists(CHROMA_ROOT_DIR):
        return
    
    dirs = []
    for item in os.listdir(CHROMA_ROOT_DIR):
        item_path = os.path.join(CHROMA_ROOT_DIR, item)
        if os.path.isdir(item_path):
            create_time = os.path.getctime(item_path)
            dirs.append((item_path, create_time))
    
    dirs.sort(key=lambda x: x[1], reverse=True)
    to_delete = dirs[keep_latest:] if keep_latest > 0 else dirs
    
    for dir_path, _ in to_delete:
        try:
            shutil.rmtree(dir_path)
            print(f"ğŸ§¹ Cleaned Chroma temp dir: {dir_path}")
        except Exception as e:
            print(f"âŒ Failed to clean {dir_path}: {e}")

# ============ åˆå§‹åŒ–LLMå’ŒåµŒå…¥æ¨¡å‹ï¼ˆæ‡’åŠ è½½ï¼‰ ============
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
import streamlit as st

# ============ Azure OpenAI Configï¼ˆé€šè¿‡ Secrets å®‰å…¨è¯»å–ï¼‰ ============
def get_azure_config():
    """ä» st.secrets å®‰å…¨è·å– Azure é…ç½®"""
    try:
        return {
            "api_key": st.secrets["AZURE_OPENAI_API_KEY"],
            "endpoint": st.secrets["AZURE_OPENAI_ENDPOINT"],
            "api_version": st.secrets.get("OPENAI_API_VERSION", "2023-05-15"),
            "chat_deployment": st.secrets.get("CHAT_DEPLOYMENT", "gpt-4o"),
            "embedding_deployment": st.secrets.get("EMBEDDING_DEPLOYMENT", "text-embedding-ada-002"),
        }
    except Exception as e:
        st.error("âŒ æœªæ£€æµ‹åˆ° Azure OpenAI é…ç½®ï¼è¯·åœ¨ Streamlit Secrets ä¸­è®¾ç½®ç›¸å…³å¯†é’¥ã€‚")
        raise e

# ============ åˆå§‹åŒ–LLMå’ŒåµŒå…¥æ¨¡å‹ï¼ˆæ‡’åŠ è½½ + Secretsï¼‰ ============
_llm: Optional[AzureChatOpenAI] = None
_embeddings: Optional[AzureOpenAIEmbeddings] = None

def get_llm():
    global _llm
    if _llm is None:
        config = get_azure_config()
        _llm = AzureChatOpenAI(
            azure_endpoint=config["endpoint"],
            azure_deployment=config["chat_deployment"],
            api_version=config["api_version"],
            api_key=config["api_key"],
            temperature=0.7,
            max_tokens=4000,
            timeout=180,
            max_retries=3,
        )
    return _llm

def get_embeddings():
    global _embeddings
    if _embeddings is None:
        config = get_azure_config()
        _embeddings = AzureOpenAIEmbeddings(
            azure_endpoint=config["endpoint"],
            azure_deployment=config["embedding_deployment"],
            api_version=config["api_version"],
            api_key=config["api_key"],
            request_timeout=60,
            max_retries=3,
        )
    return _embeddings

# ============ æ—¥æœŸå¤„ç†+æƒ…æ„Ÿå¼‚åŠ¨æ£€æµ‹å·¥å…·å‡½æ•° ============
def parse_timestamp_to_date(timestamp_input) -> str:
    """å…¼å®¹datetimeå¯¹è±¡/å­—ç¬¦ä¸²çš„æ—¶é—´è§£æ"""
    if not timestamp_input:
        return "unknown_date"
    
    # å¦‚æœæ˜¯datetimeå¯¹è±¡ï¼Œç›´æ¥æ ¼å¼åŒ–
    if isinstance(timestamp_input, datetime):
        return timestamp_input.strftime("%Y-%m-%d")
    
    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
    try:
        for fmt in ["%Y-%m-%d", "%Y-%m-%d %H:%M:%S", "%Y%m%dT%H%M%S"]:
            return datetime.strptime(str(timestamp_input)[:10], fmt).strftime("%Y-%m-%d")
    except:
        return "unknown_date"

def group_comments_by_date(social_data: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    """æŒ‰æ—¥æœŸåˆ†ç»„ç¤¾äº¤è¯„è®ºæ•°æ®ï¼ˆé€‚é…data_collectorçš„è¾“å‡ºï¼‰"""
    date_groups = defaultdict(list)
    for item in social_data:
        # ä¼˜å…ˆè¯»å–date_strï¼ˆdata_collectorå·²ç”Ÿæˆï¼‰ï¼Œæ— åˆ™è§£ætime_published
        date_str = item.get("date_str") or parse_timestamp_to_date(item.get("time_published"))
        if date_str != "unknown_date":
            date_groups[date_str].append(item)
    sorted_dates = sorted(date_groups.keys())
    return {date: date_groups[date] for date in sorted_dates}

def detect_sentiment_anomalies(social_data: List[Dict[str, Any]], threshold: float = 0.2) -> List[Dict[str, Any]]:
    """æ£€æµ‹æƒ…æ„Ÿåˆ†æ•°å¼‚åŠ¨ï¼ˆä¿®å¤KeyErrorï¼Œé€‚é…data_collectorè¾“å‡ºï¼‰"""
    # è½¬æ¢ä¸ºDataFrameå¹¶ç¡®ä¿å…³é”®åˆ—å­˜åœ¨
    df = pd.DataFrame(social_data)
    
    # æ ¸å¿ƒä¿®å¤ï¼šè¡¥å……ç¼ºå¤±çš„åˆ—ï¼Œé€‚é…data_collectorçš„é”®å
    if "time_published" in df.columns:
        df["timestamp"] = df["time_published"]  # å…¼å®¹æ—§é€»è¾‘ï¼Œæ˜ å°„ä¸ºtimestamp
    elif "date_str" in df.columns:
        df["timestamp"] = df["date_str"]
    else:
        return []  # æ— æ—¶é—´æ•°æ®ï¼Œç›´æ¥è¿”å›ç©º
    
    # ç¡®ä¿sentimentåˆ—å­˜åœ¨
    if "sentiment" not in df.columns:
        return []
    
    # è§£ææ—¥æœŸ
    df["date_str"] = df["timestamp"].apply(parse_timestamp_to_date)
    df = df[df["date_str"] != "unknown_date"]
    
    # æŒ‰æ—¥æœŸèšåˆ
    daily_sent = df.groupby("date_str")["sentiment"].agg(["median", "count"]).reset_index()
    daily_sent.columns = ["date", "avg_sentiment", "comment_count"]
    daily_sent = daily_sent.sort_values("date")
    
    # è¿‡æ»¤è¯„è®ºæ•°è¿‡å°‘çš„æ—¥æœŸ
    daily_sent = daily_sent[daily_sent["comment_count"] >= 5]
    if len(daily_sent) < 2:
        return []
    
    # è®¡ç®—æ¯æ—¥æƒ…æ„Ÿå˜åŒ–
    daily_sent["sent_change"] = daily_sent["avg_sentiment"].diff()
    daily_sent["abs_change"] = daily_sent["sent_change"].abs()
    
    # è¯†åˆ«å¼‚åŠ¨
    anomalies = daily_sent[daily_sent["abs_change"] >= threshold].to_dict("records")
    
    # æ ‡è®°å¼‚åŠ¨ç±»å‹
    for anomaly in anomalies:
        if anomaly["sent_change"] > 0:
            anomaly["type"] = "surge"
            anomaly["type_cn"] = "æƒ…æ„Ÿæš´æ¶¨"
        else:
            anomaly["type"] = "plunge"
            anomaly["type_cn"] = "æƒ…æ„Ÿæš´è·Œ"
    
    return anomalies
